#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Take a file with payloads and try to download them. Collects activity in an
`activity` folder and malware in a `malware` folder sorted by hashes.

Example: mwdk -c bcix-nt
"""

__maintainer__ = "Raphael Hiesgen"
__email__ = "raphael.hiesgen@haw-hamburg.de"
__copyright__ = "Copyright 2018-2021"

import click
import csv
import gzip
import hashlib
import ipaddress
import json
import os
import requests
import socket

from datetime import datetime, timedelta, timezone
from kafka import KafkaConsumer
from pathlib import Path

# from tldextract import extract
# # extracts abc, hostname, com
# tsd, td, tsu = extract("http://abc.hostname.com/somethings/anything/")
# url = td + '.' + tsu # will prints as hostname.com
# print url


# -- time ---------------------------------------------------------------------


def stringify(timeobject):
    utc_time = timeobject.astimezone(timezone.utc)
    return utc_time.strftime("%Y.%m")


# -- info ---------------------------------------------------------------------


def reverse_dns(ip):
    try:
        return socket.gethostbyaddr(str(ip))[0]
    except Exception as e:
        return "failed"


# -- download -----------------------------------------------------------------

inet_prefix = ipaddress.ip_network("141.22.28.0/24")
requesttimeout = 10


def is_ip_address(host):
    try:
        addr_only = host
        if ":" in host:
            addr_only = host.split(":")[0]
        _ = ipaddress.ip_address(addr_only)
        return True
    except (ipaddress.AddressValueError, ValueError):
        return False


def download(url, server):
    # Returns: malware, hashvalue, success, timestamp
    timestamp = datetime.now().astimezone(timezone.utc)
    try:
        # Need URLs to start with "http".
        if not url.startswith("http"):
            url = "http://" + url
        print(f"[DL] requesting {url}")
        r = requests.get(url, allow_redirects=True, timeout=requesttimeout, stream=True)
        r.raise_for_status()

        # Sanity check. Should be handled by `raise_for_status()`.
        assertmsg = f"WTF?, unexpected status code: {str(r.status_code)}"
        assert r.status_code == 200, assertmsg

        # Check if our DNS simply returned our research group's IP ...
        if r.raw._connection.sock is None:
            print(f"[DL] sock is None when checking {server}")
        elif not is_ip_address(server):
            try:
                sockname, _ = r.raw._connection.sock.getsockname()
                ip = ipaddress.ip_address(sockname)
                if ip in inet_prefix:
                    print(f"[ERR] IP resolved to our own prefix ({ip})")
                    return None, None, "inet prefix", timestamp
            except ValueError as e:
                print(f"[DL] getsockname failed: {e}")

        # Success!
        malware = r.content
        hashvalue = hashlib.sha256(malware).hexdigest()
        status = "success"
        return malware, hashvalue, status, timestamp

    except requests.exceptions.HTTPError as errh:
        print(f"[DL] HTTP Error: {errh}")
        return None, None, str(errh), timestamp
    except requests.exceptions.ConnectTimeout as errct:
        print(f"[DL] Connect Timeout: {errct}")
        return None, None, str(errct), timestamp
    except requests.exceptions.ConnectionError as errc:
        print(f"[DL] Connection Error: {errc}")
        return None, None, str(errc), timestamp
    except requests.exceptions.Timeout as errt:
        print(f"[DL] Timeout: {errt}")
        return None, None, str(errt), timestamp
    except requests.exceptions.RequestException as err:
        print(f"[DL] Request Exception: {err}")
        return None, None, str(err), timestamp

    print(f"[DL] Should be unreachable (url: {url})")
    return None, None, "unknown", None


# -- logging ------------------------------------------------------------------

# DOWNLOADDIR/malware/HASH/{HASH.csv, malware.bin}
# HASH.csv
# name, url, download ts, server, event type
# TODO: Do we want server here?
hash_cnames = ["name", "url", "timestamp", "server", "tag"]


def log_hash(workingdir, row, mwbytes, hashvalue):
    # Create directory.
    hashstr = str(hashvalue)
    hashdir = os.path.join(workingdir, "malware", hashstr)
    if not Path(hashdir).is_dir():
        os.makedirs(hashdir)
    # Save malware.
    malwarepath = os.path.join(workingdir, "malware", hashstr, "malware.bin")
    if not Path(malwarepath).is_file():
        with open(malwarepath, "wb") as fh:
            print(f"[MALWARE] saving malware with hash '{hashstr}'")
            fh.write(mwbytes)
    # Append log.
    logpath = os.path.join(workingdir, "malware", hashstr, "downloads.csv.gz")
    isexistingfile = Path(logpath).is_file()
    with gzip.open(logpath, "at") as fh:
        writer = csv.DictWriter(
            fh, delimiter="|", fieldnames=hash_cnames, extrasaction="ignore"
        )
        if not isexistingfile:
            writer.writeheader()
        writer.writerow(row)


# DOWNLOADDIR/NAME.ts.csv.gz
# NAME.ts.csv.gz
# ts, saddr, daddr, sport, dport, event type, url, server, rdns, attempted download, status, hash
name_cnames = [
    "ts",
    "saddr",
    "daddr",
    "sport",
    "dport",
    "tag",
    "url",
    "rdns",
    "attempted",
    "status",
    "hash",
]


def log_name(workingdir, currenttime, row):
    name = row["name"]
    if name is None or name == "":
        name = row["server"]
    formatedtime = stringify(currenttime)
    filename = f"{name}.{formatedtime}.csv.gz"
    dn = os.path.join(workingdir, "activity")
    if not Path(dn).is_dir():
        os.makedirs(dn)
    fn = os.path.join(workingdir, "activity", filename)
    isexistingfile = Path(fn).is_file()
    with gzip.open(fn, "at") as fh:
        writer = csv.DictWriter(
            fh, delimiter="|", fieldnames=name_cnames, extrasaction="ignore"
        )
        if not isexistingfile:
            writer.writeheader()
        writer.writerow(row)


# -- main ---------------------------------------------------------------------


@click.command()
@click.option(
    "-c",
    "--consume",
    "consume_me",
    type=str,
    default=None,
    multiple=True,
    help="match phases for this datasource",
)
@click.option(
    "-k",
    "--kafka-port",
    "kafka_port",
    type=int,
    default=9092,
    help="port of the local kafka server (default: 9092)",
)
@click.option(
    "-w",
    "--working-dir",
    "path",
    type=click.Path(exists=True),
    default=".",
    help="data directory for database and results (.)",
)
def main(consume_me, kafka_port, path):

    kafka_consumer_topics = ["cse2.malware.cleaned"]
    if consume_me is not None and len(consume_me) > 0:
        kafka_consumer_topics = [f"cse2.malware.cleaned.{ds}" for ds in consume_me]

    print(f"consuming from '{kafka_consumer_topics}'")

    if not Path(path).is_dir():
        print("please select a directory with --working-dir")
        return

    consumer = KafkaConsumer(
        group_id="cse2.mw.downloading",
        bootstrap_servers=[f"localhost:{kafka_port}"],
        # value_deserializer=lambda x: json.loads(x.decode("utf-8")),
    )
    consumer.subscribe(kafka_consumer_topics)

    # input_keys = [
    #     "ts",
    #     "tag",
    #     "saddr",
    #     "daddr",
    #     "sport",
    #     "dport",
    #     "payload",
    #     "tool",
    #     "decoded",
    #     "url",
    #     "server",
    #     "port",
    #     "name",
    # ]

    # We shouldn't span the servers with out downloads.
    expiretimes = {}
    downloadinterval = timedelta(hours=1)
    cleanupinterval = 100000
    cleanupcounter = cleanupinterval

    while True:
        for msg in consumer:
            # print()
            # print(f"[DBG] msg = {msg}")
            event = json.loads(msg.value.decode("utf-8"))
            # print(f"[DBG] event = {event}")
            now = datetime.now().astimezone(timezone.utc)
            url = event["url"]
            server = event["server"]
            # Check if we already downloaded this URL within our interval.
            if url in expiretimes:
                expiresat = expiretimes[url]
                if now < expiresat:
                    event["attempted"] = False
                    event["status"] = None
                    event["hashvalue"] = None
                    event["rdns"] = None
                    log_name(path, now, event)
                    continue
            malware, hashvalue, status, timestamp = download(url, server)
            event["attempted"] = True
            event["status"] = status
            event["hash"] = hashvalue
            event["timestamp"] = timestamp
            event["rdns"] = reverse_dns(event["server"])
            # Log NAME.csv
            log_name(path, now, event)
            if malware is not None:
                # Log HASH.csv
                log_hash(path, event, malware, hashvalue)
            # Ok, save the download time.
            expiretimes[url] = timestamp + downloadinterval
            cleanupcounter -= 1
            if cleanupcounter == 0:
                # Rmove all expired entries.
                keystoremove = []
                now = datetime.now().astimezone(timezone.utc)
                for key, expiresat in expiretimes.items():
                    if now >= expiresat:
                        keystoremove.append(key)
                for key in keystoremove:
                    del expiretimes[key]
                cleanupcounter = cleanupinterval


if __name__ == "__main__":
    main()
