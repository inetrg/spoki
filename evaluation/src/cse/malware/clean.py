#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Cleanup filtered logs and extract the malware URLs for a later download.

Example: mw-clean -c bcix-nt -p bcix-nt
"""

__maintainer__ = "Raphael Hiesgen"
__email__ = "raphael.hiesgen@haw-hamburg.de"
__copyright__ = "Copyright 2018-2021"

import click
import json

from kafka import KafkaProducer, KafkaConsumer

from cse.malware.payloadcleaner import PayloadCleaner


# -- main ---------------------------------------------------------------------


@click.command()
@click.option(
    "-c",
    "--consume",
    "consume_me",
    type=str,
    default=None,
    multiple=True,
    help="match phases for this datasource",
)
@click.option(
    "-p",
    "--produce",
    "produce_me",
    type=str,
    default=None,
    help="match phases for this datasource",
)
# @click.argument(
#     "-d",
#     "--datasource",
#     "datasource",
#     help="datasources to process",
#     type=str,
#     required=True,
# )
@click.option(
    "-k",
    "--kafka-port",
    "kafka_port",
    type=int,
    default=9092,
    help="port of the local kafka server (default: 9092)",
)
@click.option(
    "--kafka-batch-size",
    "kafka_batch_size",
    type=int,
    default=1,
    help="batch size for sending produced events (default: 1)",
)
@click.option(
    "--log-name",
    "error_log",
    type=str,
    default=None,
    help="Write error log for events that could not be parsed",
)
def main(consume_me, produce_me, kafka_port, kafka_batch_size, error_log):

    kafka_consumer_topics = ["cse2.malware.filtered"]
    if consume_me is not None and len(consume_me) > 0:
        kafka_consumer_topics = [f"cse2.malware.filtered.{ds}" for ds in consume_me]
    kafka_producer_topic = "cse2.malware.cleaned"
    if produce_me is not None and len(produce_me) > 0:
        kafka_producer_topic += f".{produce_me}"
    kafka_group_id = "cse2.mw.cleaning"

    print(f"consuming from '{kafka_consumer_topics}'")
    print(f"publishing to '{kafka_producer_topic}'")

    consumer = KafkaConsumer(
        group_id=kafka_group_id,
        bootstrap_servers=[f"localhost:{kafka_port}"],
        # value_deserializer=lambda x: json.loads(x.decode("utf-8")),
    )
    consumer.subscribe(kafka_consumer_topics)

    producer = KafkaProducer(
        bootstrap_servers=[f"localhost:{kafka_port}"],
        batch_size=kafka_batch_size,
        value_serializer=lambda x: json.dumps(x).encode("utf-8"),
    )

    # input_keys = [
    #     "ts",
    #     "tag",
    #     "saddr",
    #     "daddr",
    #     "sport",
    #     "dport",
    #     "payload",
    #     "tool",
    #     "decoded",
    # ]

    # output_keys = [
    #     "ts",
    #     "tag",
    #     "saddr",
    #     "daddr",
    #     "sport",
    #     "dport",
    #     "payload",
    #     "tool",
    #     "decoded",
    #     "url",     # new
    #     "server",  # new
    #     "port",    # new
    #     "name",    # new
    # ]

    while True:
        for msg in consumer:
            # print()
            # print(f"[DBG] msg = {msg}")
            event = json.loads(msg.value.decode("utf-8"))
            # print(f"[DBG] event = {event}")
            pl = event["decoded"]

            pc = PayloadCleaner(pl, error_log)
            urls = pc.get_urls()

            for info in urls:
                event["tool"] = info["tool"]
                event["url"] = info["url"]
                event["server"] = info["server"]
                event["port"] = info["port"]
                event["name"] = info["name"]
                producer.send(kafka_producer_topic, event)

if __name__ == "__main__":
    main()
