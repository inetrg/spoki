#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Query our malware hashes in Virus Total to get some additional information.
"""

__maintainer__ = "Raphael Hiesgen"
__email__ = "raphael.hiesgen@haw-hamburg.de"
__copyright__ = "Copyright 2018-2021"


import argparse
import csv
import gzip
import json
import os
import pause
import sys

from datetime import datetime, timezone
from dateutil import parser
from pathlib import Path

from cse.malware.database.virustotal import (
    VirusTotal,
    APIError,
    RateLimitReached,
    RequestsError,
)

vt_key_env = "VT_API_KEY"
download_log = "downloads.csv.gz"
databasefn = "database.json.gz"
notinvtfn = "notinvt.json.gz"

# Saves all performed queries. Our time and the VT time.
database = {}
# entry = {
#   'first': {
#       'vt': TS,
#       'spoki': TS,
#   },
#   'last': {
#       'vt': TS,
#       'spoki': TS,
#   },
#   'queries': TS
# }

notinvt = {}


def get_our_timestamps(hash, hashdir):
    fn = os.path.join(hashdir, hash, download_log)
    if not Path(fn).is_file():
        print(f"could not locate logfile for '{hash}' expected at '{fn}'")
        return
    timestamps = set()
    # names = set()  # We could collect names to compare them, ...
    with gzip.open(fn, "rt") as fh:
        reader = csv.DictReader(fh, delimiter="|")
        for line in reader:
            datetimestr = line["timestamp"]
            # ts = datetime.strptime(datetimestr, "%Y-%m-%d %H:%M:%S.%f%z")
            ts = parser.parse(datetimestr)
            timestamps.add(ts)
    if len(timestamps) == 0:
        print(f"log file for '{hash}' is empty")
        return
    timestamps = sorted(list(timestamps))
    smallest = timestamps[0]
    largest = timestamps[-1]
    for timestamp in timestamps:
        assert smallest <= timestamp, "order is wrong"
        assert largest >= timestamp, "order is wrong"
    return smallest, largest


def to_unixtimestamp(ts):
    return int(ts.astimezone(timezone.utc).timestamp())


def now():
    return to_unixtimestamp(datetime.now().astimezone(timezone.utc))


def today():
    return datetime.now().astimezone(timezone.utc).date()


def dump_reply(hash, obj, folder, overwrite=True):
    fn = f"{hash}.json.gz"
    fp = os.path.join(folder, fn)
    if not overwrite:
        if Path(fp).is_file():
            return
    with gzip.open(fp, "wt") as fh:
        json.dump(obj, fh)


def main():
    global database
    global notinvt

    parser = argparse.ArgumentParser(description="Check hashes agains VT.")
    parser.add_argument("-H", "--hash-dir", type=str, default="malware")
    parser.add_argument("-R", "--replies-dir", type=str, default="virustotal")
    parser.add_argument("-A", "--api-version", type=str, default="v3")
    parser.add_argument("-S", "--sleep-interval", type=int, default=600)
    parser.add_argument("-O", "--query-tokens-online", action="store_true")

    args = parser.parse_args()

    hsd = args.hash_dir
    rpd = args.replies_dir

    if not Path(hsd).is_dir():
        print("please select a directory with -H/--hash-dir (default: 'malware')")
        return

    if not Path(rpd).is_dir():
        print("please select a directory with --replies-dir (default: 'virustotal')")
        return

    apiversion = args.api_version
    assert apiversion == "v3", "Only API v3 is supported."
    replyfolder = os.path.join(rpd, apiversion)
    if not Path(replyfolder).is_dir():
        os.makedirs(replyfolder)

    if vt_key_env not in os.environ:
        print(f"[ERR] could not find VT API key. please set {vt_key_env}")
        return

    vtkey = os.environ[vt_key_env]
    print(f"VT key is '{vtkey}'")

    if Path(databasefn).is_file():
        print("found existing database")
        with gzip.open(databasefn, "rt", encoding="UTF-8") as fh:
            data = fh.read()
            database = json.loads(data)
            print(f"read database with {len(database)} hashes")

    if Path(notinvtfn).is_file():
        print("found existing hashes we could not query")
        with gzip.open(notinvtfn, "rt", encoding="UTF-8") as fh:
            data = fh.read()
            notinvt = json.loads(data)
            print(f"read database with {len(notinvt)} hashes")

    # Create our querier.
    vt = VirusTotal(vtkey, apiversion=apiversion)
    vt.load_stats(args.query_tokens_online)

    last_recheck = None

    def havent_checked_today():
        nonlocal last_recheck
        if last_recheck is None:
            return True
        else:
            return today() > last_recheck

    def mark_recheck():
        nonlocal last_recheck
        last_recheck = today()

    try:
        while True:
            # Read all hashes
            hashes = []

            files = os.listdir(hsd)
            # print(f"found files in '{hsd}': {files}")
            print(f"found {len(files)} files in '{hsd}'")

            for fn in files:
                joined = os.path.join(hsd, fn)
                if Path(joined).is_dir():
                    hashes.append(fn)
            print(f"{len(hashes)} are folders and should be hashes")

            databasechanged = False
            notinvtchanged = False

            def handle_reply(obj):
                nonlocal databasechanged
                nonlocal notinvtchanged
                if "data" not in obj:
                    dump_reply(hash, obj, replyfolder, False)
                    print(f"results looks malformed: no 'data' member in JSON")
                    notinvt[hash] = now()
                    notinvtchanged = True
                else:
                    dump_reply(hash, obj, replyfolder, True)
                    data = obj["data"]
                    if len(data) == 0:
                        print(f"no entry for '{hash}'")
                        notinvt[hash] = now()
                        notinvtchanged = True
                    else:
                        data = data[0]
                        print(f"got data (len = {len(data)})")
                        attributes = data["attributes"]
                        if "first_submission_date" not in attributes:
                            print("expected key 'first_submission_date' not found")
                            print(json.dumps(attributes))
                        else:
                            vt_ts_first = int(attributes["first_submission_date"])
                            vt_ts_last = int(attributes["first_submission_date"])
                            database[hash]["first"]["vt"] = vt_ts_first
                            database[hash]["last"]["vt"] = vt_ts_last
                            databasechanged = True
                            if hash in notinvt:
                                del notinvt[hash]
                                notinvtchanged = True

            def dump_databases():
                nonlocal databasechanged
                nonlocal notinvtchanged
                if databasechanged:
                    with gzip.open(databasefn, "wt", encoding="UTF-8") as fh:
                        json.dump(database, fh)
                # notinvtfn = "notinvt.json.gz"
                if notinvtchanged:
                    with gzip.open(notinvtfn, "wt", encoding="UTF-8") as fh:
                        json.dump(notinvt, fh)

            encountered_new_hash = False
            # Check all hashes.
            for hash in hashes:
                # Reset these. Not sure if want to update dump the databases in
                # each loop, but it might take a while otherwise.
                databasechanged = False
                notinvtchanged = False
                our_ts_first, our_ts_last = get_our_timestamps(hash, hsd)
                if hash in database:
                    # print(f"skipping '{hash}' because it's already in the database")
                    tmp_ts = to_unixtimestamp(our_ts_last)
                    if tmp_ts != database[hash]["last"]["spoki"]:
                        database[hash]["last"]["spoki"] = tmp_ts
                        databasechanged = True
                else:
                    encountered_new_hash = True
                    database[hash] = {
                        "first": {"spoki": to_unixtimestamp(our_ts_first)},
                        "last": {"spoki": to_unixtimestamp(our_ts_last)},
                    }
                    databasechanged = True
                    try:
                        obj = vt.search(hash, blocking=True)
                        handle_reply(obj)
                    except APIError as e:
                        print(e)
                    except RequestsError as e:
                        print(e)
                    except RateLimitReached as e:
                        print(e)

                # TODO: Check if we have "access tokens".
                # if so --> use_up_tokens()

                # TODO: Rate limit disk writes?
                # Write database and notinvt to disk ...
                dump_databases()

            # If we have access tokens, let's use them!
            if vt.can_go_at_full_speed() and havent_checked_today():
                print(f"rechecking on '{today()}'', last checked on '{last_recheck}'")
                mark_recheck()
                tokens = vt.remaining_this_day()
                use = 0
                if tokens < 10:
                    use = tokens
                else:
                    use = int(tokens * 0.8)
                pending = list(notinvt.items())[:use]
                print(f"will re-query {len(pending)} of {len(notinvt)} hashes")
                for hash, _ in pending:
                    obj = vt.search(hash, blocking=True)
                    handle_reply(obj)
                dump_databases()
            print(f"tokens remaining today: {vt.remaining_this_day()}")
            if not encountered_new_hash:
                secs = args.sleep_interval
                print(f"going to sleep for {secs}s")
                pause.seconds(secs)
    except KeyboardInterrupt:
        vt.save_stats()
        sys.exit(0)


if __name__ == "__main__":
    main()
