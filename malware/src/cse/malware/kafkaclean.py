#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Cleanup filtered logs and extract the malware URLs for a later download.

Example: mw-clean -c bcix-nt -p bcix-nt
"""

__maintainer__ = "Raphael Hiesgen"
__email__ = "raphael.hiesgen@haw-hamburg.de"
__copyright__ = "Copyright 2018-2021"

import click
import json

from collections import defaultdict
from kafka import KafkaProducer, KafkaConsumer
from urllib.parse import unquote_plus, urlparse


# -- payload processing -------------------------------------------------------


def clean_pl(pl):
    pl = pl.replace(";", " ")
    pl = pl.replace("`", " ")
    pl = pl.replace("+", " ")
    pl = pl.replace("${IFS}", " ")
    pl = pl.replace("\r", " ")
    pl = pl.replace("\n", " ")
    pl = pl.replace("\t", " ")
    pl = pl.replace("'", " ")
    pl = pl.replace("(", " ")
    pl = pl.replace("\\", "")
    pl = pl.replace("http:// ", "http://")
    pl = pl.replace("  ", " ")
    pl = pl.replace("&", " ")
    return pl


def extract(pl, call):
    try:
        pl = pl[pl.find(call) :]
        parts = pl.split(" ")
        idx = 0
        args = []
        while parts[idx + 1].startswith("-"):
            args.append(parts[idx + 1])
            args.append(parts[idx + 2])
            idx += 2
        url = parts[idx + 1]
        if url in ["|", ";"]:
            url = ""
        if len(args) > 0:
            url = " ".join(args) + " " + url
            # print(f'{call},  url = "{url}" [ARGS]')
        else:
            # print(f'{call},  url = {url}')
            pass
        # print(url)
        return call, url
    except IndexError:
        return call, ""


def is_url(x):
    try:
        result = urlparse(x)
        return all([result.scheme, result.netloc, result.path])
    except Exception:
        return False


def parse_wget_args(args):
    arguments = args.strip().split(" ")
    buildingblocks = defaultdict(lambda: "")
    # Get data from options (busybox wget)
    count = len(arguments)
    idx = 0
    while idx < count:
        current = arguments[idx].strip()
        print(f"[WGET] next arugment: '{current}'")
        idx += 1
        if not current.startswith("-"):
            # Should be the host, other options have an argument starting
            # with '-'.
            print(f"[WGET] host is '{current}'")
            if "host" not in buildingblocks:
                buildingblocks["host"] = current
            else:
                existing = buildingblocks["host"]
                if current.startswith("http://") and not existing.startswith("http://"):
                    buildingblocks["host"] = current
                elif is_url(current) and not is_url(existing):
                    buildingblocks["host"] = current
        else:
            if current == "-g":
                # This is a download.
                print("[WGET] is a download")
                pass
            elif current == "-l":
                if idx < count:
                    buildingblocks["local_path"] = arguments[idx].strip()
                    print(f"[WGET] local path: '{buildingblocks['local_path']}'")
                    idx += 1
            elif current == "-r":
                if idx < count:
                    buildingblocks["remote_path"] = arguments[idx].strip()
                    print(f"[WGET] remote path: '{buildingblocks['remote_path']}'")
                    idx += 1
            else:
                print(f'unknown wget option: "{current}"')
                pass
    # Check if we have enough to build the URL
    if buildingblocks["host"] == "":
        print(f"could not build URL from {args}")
        return None
    # Build URL from host and remote path
    url = "http://"
    url += buildingblocks["host"]
    rpath = buildingblocks["remote_path"]
    if not url.endswith("/") and not rpath.startswith("/"):
        url += "/"
    url += rpath
    return url


def parse_curl_args(args):
    parts = args.split(" ")
    buildingblocks = defaultdict(lambda: "")
    for idx in range(0, len(parts), 2):
        # print(f'checking index {idx}')
        opt = parts[idx]
        if not opt.startswith("-"):
            # print(f' no longer an argument, skipping rest: {parts[idx +1:]}')
            continue
        arg = parts[idx + 1]
        if opt == "-O":
            if "host" not in buildingblocks:
                buildingblocks["host"] = arg
            else:
                existing_arg = buildingblocks["host"]
                if arg.startswith("http://") and not existing_arg.startswith("http://"):
                    buildingblocks["host"] = arg
                elif is_url(arg) and not is_url(existing_arg):
                    buildingblocks["host"] = arg
        elif opt == "-A" or opt == "--user-agent":
            # Currently ignored, but could be important for downloads?
            buildingblocks["useragent"] = arg
        # TODO: Check other long options.
        elif len(opt) > 2 and not opt.startswith("--"):
            # This should parse options like -sSL or similar.
            options = opt[1:]
            host_is_next = True
            for c in options:
                if c in ["f", "s", "S", "L"]:
                    # f: fail silently, don't care
                    # s: silent mode, don't care
                    # S: Show error if it fails, don't care
                    # L: location, i.e., "If  the  server  reports that the
                    #   requested page has moved to a different location [...]
                    #   this option will make curl redo the request on the new
                    #   place."
                    continue
                else:
                    print(f"uknown curl short opt: {c}")
            if host_is_next:
                buildingblocks["host"] = arg
        else:
            print(f'unknown option: "{opt}"')
            continue
    # Check if we have enough to build the URL
    if (
        "host" not in buildingblocks
        or buildingblocks["host"] is None
        or buildingblocks["host"] == ""
    ):
        print(f"could not build URL from {args}")
        return None
    # Build URL from host and remote path
    url = buildingblocks["host"]
    if not url.startswith("http"):
        url = "http://" + url
    if "remote_path" in buildingblocks:
        rpath = buildingblocks["remote_path"]
        if not url.endswith("/") and not rpath.startswith("/"):
            url += "/"
        url += rpath
    return url


def extract_name(url):
    name = None
    if "/" in url:
        parts = url.split("/")
        name = parts[-1]
    else:
        print(f"[NAME] encountered URL without name: {url}")
        name = url  # TODO: add a fake name
    return name


# -- main ---------------------------------------------------------------------


@click.command()
@click.option(
    "-c",
    "--consume",
    "consume_me",
    type=str,
    default=None,
    multiple=True,
    help="match phases for this datasource",
)
@click.option(
    "-p",
    "--produce",
    "produce_me",
    type=str,
    default=None,
    help="match phases for this datasource",
)
# @click.argument(
#     "-d",
#     "--datasource",
#     "datasource",
#     help="datasources to process",
#     type=str,
#     required=True,
# )
@click.option(
    "-k",
    "--kafka-port",
    "kafka_port",
    type=int,
    default=9092,
    help="port of the local kafka server (default: 9092)",
)
@click.option(
    "--kafka-batch-size",
    "kafka_batch_size",
    type=int,
    default=1,
    help="batch size for sending produced events (default: 1)",
)
def main(consume_me, produce_me, kafka_port, kafka_batch_size):

    kafka_consumer_topics = ["cse2.malware.filtered"]
    if consume_me is not None and len(consume_me) > 0:
        kafka_consumer_topics = [f"cse2.malware.filtered.{ds}" for ds in consume_me]
    kafka_producer_topic = "cse2.malware.cleaned"
    if produce_me is not None and len(produce_me) > 0:
        kafka_producer_topic += f".{produce_me}"
    kafka_group_id = "cse2.mw.cleaning"

    print(f"consuming from '{kafka_consumer_topics}'")
    print(f"publishing to '{kafka_producer_topic}'")

    consumer = KafkaConsumer(
        group_id=kafka_group_id,
        bootstrap_servers=[f"localhost:{kafka_port}"],
        # value_deserializer=lambda x: json.loads(x.decode("utf-8")),
    )
    consumer.subscribe(kafka_consumer_topics)

    producer = KafkaProducer(
        bootstrap_servers=[f"localhost:{kafka_port}"],
        batch_size=kafka_batch_size,
        value_serializer=lambda x: json.dumps(x).encode("utf-8"),
    )

    # input_keys = [
    #     "ts",
    #     "tag",
    #     "saddr",
    #     "daddr",
    #     "sport",
    #     "dport",
    #     "payload",
    #     "tool",
    #     "decoded",
    # ]

    # output_keys = [
    #     "ts",
    #     "tag",
    #     "saddr",
    #     "daddr",
    #     "sport",
    #     "dport",
    #     "payload",
    #     "tool",
    #     "decoded",
    #     "url",     # new
    #     "server",  # new
    #     "port",    # new
    #     "name",    # new
    # ]

    def clean(event, tool, pl, args_parser_fun):
        tool, url = extract(pl, tool)
        if url == "http":
            url = "MISSING"
            print(f"[ERR] failed to extract URL from '{pl}'")
            return None
        if url == "":
            print(f"[PARSE] failed to extract URL from '{pl}'")
            return None

        if url.startswith("-"):
            url_parsed_from_args = args_parser_fun(url)
            if url is None:
                print(f'failed to parse "{url}')
                return None
            print(f"[PARSE] got '{url_parsed_from_args}' from '{url}' for '{tool}'")
            url = url_parsed_from_args

        if url is None or url == "":
            print(f"[PARSE] failed find URL in '{pl}'")
            return None

        if not url.startswith("http://"):
            url = "http://" + url
        server = urlparse(url).netloc

        port = 80
        if ":" in server:
            parts = server.split(":")
            if len(parts) == 2:
                server = parts[0]
                port = parts[1]
        print(f"{tool}|{url}|{server}|{port}")
        # build event
        event["tool"] = tool
        event["url"] = url
        event["server"] = server
        event["port"] = port
        event["name"] = extract_name(url)
        return event

    while True:
        for msg in consumer:
            # print()
            # print(f"[DBG] msg = {msg}")
            event = json.loads(msg.value.decode("utf-8"))
            # print(f"[DBG] event = {event}")
            pl = event["decoded"]
            pl = clean_pl(unquote_plus(pl))
            # There might be payloads that use both tools.
            found_wget_url = False
            found_curl_url = False
            # Check wget
            if "wget" in pl:
                elem = None
                elem = clean(event.copy(), "wget", pl, parse_wget_args)
                if elem is not None:
                    producer.send(kafka_producer_topic, elem)
                    found_wget_url = True
            # Check curl
            if "curl" in pl:
                elem = None
                if found_wget_url:
                    print(f"[PARSE] pl shows both tools: '{pl}'")
                if "User-Agent: curl" not in pl:
                    elem = clean(event, "curl", pl, parse_curl_args)
                    found_curl_url = True
                else:
                    duplicate = pl.replace("User-Agent: curl", "")
                    if "curl" in duplicate:
                        print(f"[PARSE] found curl in user agent and payload: '{pl}'")
                        elem = clean(event, "curl", duplicate, parse_curl_args)
                        found_curl_url = True
                if elem is not None:
                    # TODO: Check fields in dict!
                    producer.send(kafka_producer_topic, elem)
            # Warn if we didn't find anything ...
            if not found_wget_url and not found_curl_url:
                print(f"[WARN] payload doesn't contain wget/curl: '{pl}'")


if __name__ == "__main__":
    main()
